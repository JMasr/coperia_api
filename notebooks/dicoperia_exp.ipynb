{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook of COPERIA UVigo-GTM team\n",
    "# @autor: José M. Ramírez @email: jmramirez@gts.uvigo.es @date: 2023-02-27 @version: 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Define important paths\n",
    "path_data = '/home/jsanhcez/Documentos/Proyectos/06_TODO_COPERIA/repos/coperia_api/dataset_dicoperia/'\n",
    "path_wav = '/home/jsanhcez/Documentos/Proyectos/06_TODO_COPERIA/repos/coperia_api/dataset_dicoperia/wav_48000kHz/'\n",
    "\n",
    "path_feats = 'data/features/'\n",
    "path_results = 'results/'\n",
    "path_models = 'models/'\n",
    "path_notebooks = '/home/jsanhcez/Documentos/Proyectos/99_to_do_COPERIA/repos/coperia_api/notebooks'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the data to be used\n",
    "CLASSES = ['covid-control', 'covid-persistente']\n",
    "\n",
    "dicoperia_metadata = pd.read_csv(os.path.join(path_data, 'metadata_dicoperia.csv'), decimal=',')\n",
    "dicoperia_filters = {'audio_id': ['c15e54fc-5290-4652-a3f7-ff3b779bd980', '244b61cc-4fd7-4073-b0d8-7bacd42f6202'],\n",
    "                     'patient_type': ['coperia-rehab'],\n",
    "                     'audio_type': ['/a/'],\n",
    "                     'audio_moment': ['after']}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            patient_id       patient_type  \\\n0     1848560680d-b7b0077c-b059-4c72-a3d6-fc05f18739ff      covid-control   \n1     1848560680d-b7b0077c-b059-4c72-a3d6-fc05f18739ff      covid-control   \n2     1848560680d-b7b0077c-b059-4c72-a3d6-fc05f18739ff      covid-control   \n3     18486282fe2-1d4c0df2-25c3-458e-b44c-b05fc15f6ce7      covid-control   \n4     18486282fe2-1d4c0df2-25c3-458e-b44c-b05fc15f6ce7      covid-control   \n...                                                ...                ...   \n2557  185da0265a8-827181dd-f084-4c7a-9d29-3636cb176791  covid-persistente   \n2558  185da0265a8-827181dd-f084-4c7a-9d29-3636cb176791  covid-persistente   \n2559  185da0265a8-827181dd-f084-4c7a-9d29-3636cb176791  covid-persistente   \n2560  185da0265a8-827181dd-f084-4c7a-9d29-3636cb176791  covid-persistente   \n2561  185a78b242e-8eeecddb-0429-497c-b68d-c8fc76257fc0  covid-persistente   \n\n      age  gender                              audio_id  covid  long_covid  \\\n0      49  female  c6b9296a-eed4-4c54-9c80-ecf715a90b6b    NaN       False   \n1      49  female  462f6e89-c12e-4e76-9916-17748f342472    NaN       False   \n2      49  female  b4f0aa5a-2959-46c6-9fd5-5019b74b85d6    NaN       False   \n3      56  female  89cdb797-f77c-464f-8b7b-304f0d509528    NaN       False   \n4      56  female  3b7b679d-1f2d-43bc-ba13-4f870cd7299e    NaN       False   \n...   ...     ...                                   ...    ...         ...   \n2557   53  female  38c2a282-81f1-4df1-8bfd-bac15382dfd2    NaN        True   \n2558   53  female  7fb4a174-5a35-4ac3-98eb-b6a53a87a76b    NaN        True   \n2559   53  female  07882330-a75b-43df-aaed-66f3fe80f49d    NaN        True   \n2560   53  female  914a472a-4065-4247-89ce-3599f7dce9b0    NaN        True   \n2561   56  female  42b1d383-5854-4b11-b32e-ea34d59d559a    NaN        True   \n\n     audio_type audio_moment audio_code  duration  \n0       /cough/       before    84435-7     3.007  \n1       /cough/       before    84435-7     4.003  \n2       /cough/       before    84435-7     3.006  \n3       /cough/        after    84435-7     2.006  \n4       /cough/        after    84435-7     3.003  \n...         ...          ...        ...       ...  \n2557        /a/        after    84728-5     3.002  \n2558        /a/        after    84728-5     3.004  \n2559        /a/        after    84728-5     4.004  \n2560        /a/       before    84728-5     5.000  \n2561        /a/       before    84728-5    12.000  \n\n[2562 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>patient_id</th>\n      <th>patient_type</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>audio_id</th>\n      <th>covid</th>\n      <th>long_covid</th>\n      <th>audio_type</th>\n      <th>audio_moment</th>\n      <th>audio_code</th>\n      <th>duration</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1848560680d-b7b0077c-b059-4c72-a3d6-fc05f18739ff</td>\n      <td>covid-control</td>\n      <td>49</td>\n      <td>female</td>\n      <td>c6b9296a-eed4-4c54-9c80-ecf715a90b6b</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>/cough/</td>\n      <td>before</td>\n      <td>84435-7</td>\n      <td>3.007</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1848560680d-b7b0077c-b059-4c72-a3d6-fc05f18739ff</td>\n      <td>covid-control</td>\n      <td>49</td>\n      <td>female</td>\n      <td>462f6e89-c12e-4e76-9916-17748f342472</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>/cough/</td>\n      <td>before</td>\n      <td>84435-7</td>\n      <td>4.003</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1848560680d-b7b0077c-b059-4c72-a3d6-fc05f18739ff</td>\n      <td>covid-control</td>\n      <td>49</td>\n      <td>female</td>\n      <td>b4f0aa5a-2959-46c6-9fd5-5019b74b85d6</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>/cough/</td>\n      <td>before</td>\n      <td>84435-7</td>\n      <td>3.006</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>18486282fe2-1d4c0df2-25c3-458e-b44c-b05fc15f6ce7</td>\n      <td>covid-control</td>\n      <td>56</td>\n      <td>female</td>\n      <td>89cdb797-f77c-464f-8b7b-304f0d509528</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>/cough/</td>\n      <td>after</td>\n      <td>84435-7</td>\n      <td>2.006</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>18486282fe2-1d4c0df2-25c3-458e-b44c-b05fc15f6ce7</td>\n      <td>covid-control</td>\n      <td>56</td>\n      <td>female</td>\n      <td>3b7b679d-1f2d-43bc-ba13-4f870cd7299e</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>/cough/</td>\n      <td>after</td>\n      <td>84435-7</td>\n      <td>3.003</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2557</th>\n      <td>185da0265a8-827181dd-f084-4c7a-9d29-3636cb176791</td>\n      <td>covid-persistente</td>\n      <td>53</td>\n      <td>female</td>\n      <td>38c2a282-81f1-4df1-8bfd-bac15382dfd2</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>/a/</td>\n      <td>after</td>\n      <td>84728-5</td>\n      <td>3.002</td>\n    </tr>\n    <tr>\n      <th>2558</th>\n      <td>185da0265a8-827181dd-f084-4c7a-9d29-3636cb176791</td>\n      <td>covid-persistente</td>\n      <td>53</td>\n      <td>female</td>\n      <td>7fb4a174-5a35-4ac3-98eb-b6a53a87a76b</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>/a/</td>\n      <td>after</td>\n      <td>84728-5</td>\n      <td>3.004</td>\n    </tr>\n    <tr>\n      <th>2559</th>\n      <td>185da0265a8-827181dd-f084-4c7a-9d29-3636cb176791</td>\n      <td>covid-persistente</td>\n      <td>53</td>\n      <td>female</td>\n      <td>07882330-a75b-43df-aaed-66f3fe80f49d</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>/a/</td>\n      <td>after</td>\n      <td>84728-5</td>\n      <td>4.004</td>\n    </tr>\n    <tr>\n      <th>2560</th>\n      <td>185da0265a8-827181dd-f084-4c7a-9d29-3636cb176791</td>\n      <td>covid-persistente</td>\n      <td>53</td>\n      <td>female</td>\n      <td>914a472a-4065-4247-89ce-3599f7dce9b0</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>/a/</td>\n      <td>before</td>\n      <td>84728-5</td>\n      <td>5.000</td>\n    </tr>\n    <tr>\n      <th>2561</th>\n      <td>185a78b242e-8eeecddb-0429-497c-b68d-c8fc76257fc0</td>\n      <td>covid-persistente</td>\n      <td>56</td>\n      <td>female</td>\n      <td>42b1d383-5854-4b11-b32e-ea34d59d559a</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>/a/</td>\n      <td>before</td>\n      <td>84728-5</td>\n      <td>12.000</td>\n    </tr>\n  </tbody>\n</table>\n<p>2562 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicoperia_metadata"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['1848560680d-b7b0077c-b059-4c72-a3d6-fc05f18739ff',\n       '18486282fe2-1d4c0df2-25c3-458e-b44c-b05fc15f6ce7',\n       '1848b5d301b-7b953155-4e79-456c-92d0-cf3555a8c232',\n       '1848b990553-b9fd84c1-8791-4b6d-a125-1be68a472313',\n       '1848b90599c-abba7a90-0bfd-4427-b8e8-bfa862fea9c2',\n       '18490356286-f3384021-c5e9-4ee3-ba21-9a146fe7dc0f',\n       '1849150ab13-7b4c0b96-26bf-466a-a970-959f2d5c0902',\n       '1849157611a-c04a00c4-9d36-427e-a082-d6d27a882e8d',\n       '18484e121f5-97ecde04-0d12-4515-a273-e37105857128',\n       '1848b9c142b-73ef05e3-3597-4e56-b86c-801f147e819d',\n       '184915519bf-895c84da-6feb-4f80-9e07-fe06b28f15e0',\n       '1849a4b5aa7-52fafe7f-7426-421b-aa5a-670bdd771e09',\n       '1849a4d0eb9-0b6a5c17-5a1e-4540-b403-2d0f29c885f0',\n       '1849ec25906-650f1458-defc-4418-9821-2407084f72bb',\n       '1849eca8a5e-78e647d8-4255-4bf4-bd5b-593bddbc8590',\n       '1849ed39755-1554db88-deb7-464c-92e2-ca9721f39ae0',\n       '184a3cbeaa5-aab0c783-3326-42c0-aa37-7878f1c08173',\n       '184a3dad0f3-3301c71d-5f9e-431c-8530-42905cecb6df',\n       '184a3e55ee0-161abd3a-2273-4b37-af05-770e2de9b630',\n       '184adb739d4-34eb7391-b228-4ebb-9825-7b4e6ec72d36',\n       '184af048ce6-c50b68ff-58bd-49e9-8849-cd8cce588290',\n       '1849ede3c70-d6ab403e-4ad2-40bf-967b-260bbe691a0a',\n       '184a3dff2fa-81669a25-fc11-4f0e-acdc-4b44b6082cad',\n       '184ae96c13c-e3015c51-0e73-4e92-8e52-4f322075f09d',\n       '184af034b1e-4b042ae5-b373-4bbf-be5e-7a1997d40d07',\n       '184b0482677-1ce242d1-65e6-4213-b881-9e08f4760feb',\n       '184b033cc1c-edc372bc-5028-4165-b879-97787a287fa7',\n       '184b041f947-cd70f109-2503-4ae7-975a-bba7b00524bc',\n       '184b04b7014-80993752-7bef-4c70-8196-c36889192948',\n       '184b9587115-b996d866-6edc-405e-b547-e65c6c25da30',\n       '184b96f8be6-d0dc70e7-d464-40cd-8a79-9abf0a01e9b1',\n       '184b998ed7d-e6648f2c-0646-4c19-b61b-b19b44acbe23',\n       '184b9e49054-c2ff975e-e9e0-4ccd-b8f7-6b21296256ae',\n       '184b02e01f6-6ee83ca7-ba3d-44ae-8d50-435bc873f4b4',\n       '184b964af10-dc844554-bff0-408b-bbf3-4711c17347d9',\n       '184bec0cb9e-355a3981-1ebd-4541-a60e-5b00ebf15ea3',\n       '184c2719abd-4643d37b-8099-4ab5-a5d7-bedd9a298cb8',\n       '184bf9dd4d0-e7f72b0b-b9f6-4907-958e-f69ecd18ab09',\n       '184c2779335-451ed6b2-316a-46ec-9255-5fed0d4dca7a',\n       '184c2846a19-71286a24-0f7c-4a99-a019-a0b3e1fdc40a',\n       '184c2966d5e-70ece148-a093-4602-83a4-91d2d90e0ad7',\n       '184c29ac067-f24c0797-4a3f-49a9-9997-7f7dff369af1',\n       '184c2afc3c8-3bfa9af4-683d-47d4-b79b-3fa001d6056a',\n       '184c2c4cef1-b33fdc49-063e-4c31-9c81-02211edc1a7f',\n       '184c2cbc999-03adc76c-da18-416b-b4e9-0b197c1d4e88',\n       '184c2e1c29e-943e84dc-11f8-455e-90a2-5b3d9ca33cf8',\n       '184c91b8cce-47aceb2e-dd24-4c5b-8984-a82227dfddad',\n       '184c2f683df-cd56edc8-e477-4826-9ca7-96ca97090630',\n       '184c79eaf3f-fbab5cf5-b502-47f4-99d1-b0b7339a3811',\n       '184c7a21195-d2c26ecd-9e51-47c7-b532-d7b1f0c6c895',\n       '184c27ba83b-20cd7b99-f2c7-4055-ae6c-9b9107453a57',\n       '184c293a98c-3905e1f1-765e-41e6-9148-e5a33747c240',\n       '184c45cefdc-093768d6-7f96-4d60-801d-00126e65a9a2',\n       '184c465a0d4-c2e07c21-ffc0-4b34-82c1-5b90d9eec09b',\n       '184c7b1d3ec-5549a228-6950-42b2-bdd0-19a08e979a37',\n       '184c7b6336f-221ce3d2-b74e-47a5-91b7-9e30830b2da1',\n       '184c2fb6edd-002177a0-feb3-4a13-9b3f-1528c17f15c2',\n       '184c300c842-cc1930e2-7b44-4c7d-b9e4-a77ba1eb90d9',\n       '184c7ba322b-af365ef8-6acb-45ad-a54f-439416dd9bf8',\n       '184cdf7772e-f9a4a0c5-eb3f-437c-a90e-8d21a1535bcd',\n       '184d291493b-d27b6124-e9f1-4af3-8bac-7a4fac91f08e',\n       '184cdfcfd37-f55e122d-eccb-46cd-a9b5-13ad3ef31931',\n       '184ce065006-6bbf4103-7586-4a70-915f-d7c0e918a70f',\n       '184c7d67640-03b1e748-97b5-4112-9969-c73a5b440fbd',\n       '184ce009d2f-878ec8cb-6d9a-47a2-a743-ca9dd6e98c30',\n       '184ce03b420-c502afd2-2cc7-45cc-9688-5aceaee4fcf6',\n       '18533cc3886-83b830e5-aa6c-4382-a019-2fa77067b143',\n       '18533de99e6-5263e9f0-49c3-4c77-bc4e-ceddc8d5a6b4',\n       '18533fd93db-3c6821c7-9fcb-4965-ab8c-15ae9b18241f',\n       '1853404f6ca-f11bf863-5f85-4604-a097-efca0858dede',\n       '1854dea07b9-35a3a180-9344-4566-8073-aa47463b20af',\n       '1854deffbe4-1d232c69-ac1b-45a1-a78a-fe456944e30f',\n       '184cdec3359-ec190160-0ec8-4874-b657-612a4164ca66',\n       '184cdf356dc-f1139ad7-b5c8-419e-9322-14c290c7f1a6',\n       '184d3603398-e860e8a4-dcbf-4be5-b5fd-df5125342a27',\n       '184d7659ef3-1e9392ae-dd8a-42f1-9fca-14e9bea0be9c',\n       '1851aa84f22-47d9b8d3-ffa1-4ec0-b5b3-270cad335ba4',\n       '18533eb3e49-7f80cfa3-1b76-4bec-8ee2-cbda94da75ed',\n       '18552a59040-8981cb11-bf91-4f50-95fc-b1a34f67aab6',\n       '1855304b68e-e434d9db-2029-424a-9364-e9e13992621d',\n       '185588936e3-3790d9e6-9a00-4ad5-a4bc-3d5dddb6d7f2',\n       '18558979d46-724afde2-242c-424f-8ba2-c278708b70a4',\n       '18558bad4b7-b777d505-24ad-496e-903e-898541f71938',\n       '18559431aff-86457f9d-989e-44ff-bd8d-6a8d4ff36b15',\n       '185594c653e-c5c38fca-d76f-4c5a-b19e-ed786567b733',\n       '1855956045c-2ba4df17-fc1b-484c-a904-643c9e3878de',\n       '1853fa19d3b-49fea7e8-9473-4e92-b7ca-e450e3fb124c',\n       '185536a2f67-1fb7388a-d8c4-4860-b326-62705deffb65',\n       '18557a938d9-72a39617-1d64-446d-9013-57e92e0485f4',\n       '18557f68a62-c0b41f38-6d21-4e68-97d4-209c012ac025',\n       '1855847a0b7-e70e939d-05af-4688-bf47-69d49e65fb74',\n       '18562d7b6c4-df58d3ed-b110-4d18-815f-19e4e52f63ab',\n       '1855e849034-755f78b0-6946-429f-9568-0aae0cca28ab',\n       '18562e498d1-b4582863-4dc5-4c71-a78f-4f8b76cf159a',\n       '18559875534-384eb6b6-811a-4e07-888b-687e432e90fc',\n       '18559935234-d4401b48-536f-445d-9b0e-11471b21725d',\n       '18562dd9f9a-0545f575-83ac-4a04-be0d-0eee55de87d7',\n       '18580dc89f7-d50674b0-3945-432d-9a37-01fb4dd868be',\n       '1859566e746-7fdcc049-8457-4567-9e6e-3bb16fd585ee',\n       '18595744b5f-fc10d7e6-f7b7-4e66-86c6-59065fe9af7f',\n       '1859aaa2e5f-1ca68d28-010e-46f7-b60d-c357e836489c',\n       '1859ab33b9e-271bad36-f283-451e-bb38-81c590b9739d',\n       '185a4e25d97-bbbdee6e-928a-446f-8520-aa3e0824e6ab',\n       '185722046b3-ae8e98f4-c030-423a-bf12-9feaafa24b23',\n       '1857c0ad3c9-322605f0-5cd8-42fe-a457-41227beabd6e',\n       '1857c08fd32-cab74a3f-91e9-4868-bac4-a359765fc124',\n       '18580e06a12-38945ab3-366e-43af-929d-be5e15de49a2',\n       '185830ce449-ec9a36db-d218-45f0-a37a-b92952e0af6a',\n       '185831a80af-9801ab18-7b10-4d0a-8f17-0a3af4bbd30d',\n       '18583255b33-2cf6c67c-3d64-4a29-bd2e-a437e26838ee',\n       '1859a8db9d0-4f460e50-4a54-40e7-bd3e-d851641f0da5',\n       '1859a95eaff-3911a81c-6272-4e7e-9349-dde1d6d97a2b',\n       '1858300c720-11fd0760-68c2-4d3e-8036-5a4d9d064ad2',\n       '185a76c96d1-860f904e-74d0-4e77-97d0-40e2e2eff897',\n       '185a4ed484f-7c3ebdb6-4b63-4829-938d-79c77e7c8ff5',\n       '185a776b29f-963ea9e0-a2ee-4eeb-8a3b-8c8091eb36d7',\n       '185a781c831-6aba91be-b777-4d47-90ad-eff1ed10e04d',\n       '185a78b242e-8eeecddb-0429-497c-b68d-c8fc76257fc0',\n       '185b994d55d-9e45a3d3-7e5f-426b-90d4-fdc82239f733',\n       '185b99ab49b-43613a0e-d341-49f9-ae4c-ec77bc9d63c8',\n       '185beafb897-d310a901-c29b-4c09-9ff8-c4cb1c89ceaf',\n       '185c3d5d738-ec0f2b14-460e-4331-a65b-62d87f7b67df',\n       '185bec164b3-d9853128-495f-4dca-a711-ecea7ad32a9c',\n       '18534168e6d-be505f7f-ddb5-4526-a0c9-94ab8a84675b',\n       '185c8f80714-78d630df-f36e-4e2b-ab75-59627950dc8a',\n       '185c3e06565-85a5d77e-a4ee-4bb2-8b07-677aab02a20e',\n       '185ce012e29-df3b2840-9fe5-4bc3-8215-33f6fe124f83',\n       '185cf529f54-e063d780-23eb-4818-9ccc-b8b9db99bd1d',\n       '185cdf4339a-9b70a227-7ffd-4244-a619-06bdc834afce',\n       '185cf4a48ac-6955b7bd-2aa3-48ff-8543-f061841afef0',\n       '185cf582f24-78340b7f-5a9d-4c4a-b62b-111ead107b98',\n       '185d353e138-d0414ab3-a466-426c-8054-14a5059c586b',\n       '185d35c2b49-c9e37459-db68-4f10-b34f-48ef3ba20af1',\n       '185d363d680-4a63e97e-18ca-45ef-bf48-e32319fca85c',\n       '185d9fa7040-f61a045b-0e31-4a6d-9804-598a94f2275d',\n       '185d369cd52-531e2dc0-ce73-45cb-a4f0-8873dae316de',\n       '185d95d6e0b-685ec48d-932c-4384-9b8a-bd280eacc90c',\n       '185da0265a8-827181dd-f084-4c7a-9d29-3636cb176791'], dtype=object)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patients = dicoperia_metadata['patient_id'].unique()\n",
    "patients"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering the metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 654.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering DONE!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def make_dicoperia_metadata(root_path:str, metadata: pd.DataFrame, filter: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Make a metadata file for the COPERIA dataset filtering some columns\n",
    "    :param root_path: root path of the data directory\n",
    "    :param metadata: a list with all the audio samples in COPERIA as an Audio class\n",
    "    :param filter: a dictionary with the columns and values to filter\n",
    "    :return: a pandas dataframe with the metadata of the DICOPERIA dataset\n",
    "    \"\"\"\n",
    "    print('Filtering the metadata...')\n",
    "\n",
    "    df = metadata.copy()\n",
    "    for key, values in tqdm(filter.items()):\n",
    "        df = df[~df[key].isin(values)]\n",
    "    df.to_csv(os.path.join(root_path, 'metadata_dicoperia.csv'), index=False, decimal=',')\n",
    "    print('Filtering DONE!!')\n",
    "    df.replace(CLASSES, [0, 1], inplace=True)\n",
    "    return df\n",
    "\n",
    "exp_metadata = make_dicoperia_metadata(path_notebooks, dicoperia_metadata, dicoperia_filters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Global seed\n",
    "SEED = 42\n",
    "# Define the k-folds\n",
    "K_NUM = 5\n",
    "SAMPLE_GAIN_TESTING = 0.05\n",
    "# Define the interest dataset columns\n",
    "AUDIO_ID_COLUMN = 'audio_id'\n",
    "PATIENT_ID_COLUMN = 'patient_id'\n",
    "\n",
    "CLASS_COLUMN = 'patient_type'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set: 7 patients & 21 samples\n",
      "Train-set: 131 patients & 396 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Making the subsets by patients\n",
    "patient_data = exp_metadata[[PATIENT_ID_COLUMN, CLASS_COLUMN]].drop_duplicates()\n",
    "patient_id = patient_data[PATIENT_ID_COLUMN]\n",
    "patient_class = patient_data[CLASS_COLUMN]\n",
    "\n",
    "patients_train, patients_test, patient_labels_train, patient_labels_test = train_test_split(patient_id, patient_class, test_size=SAMPLE_GAIN_TESTING, random_state=SEED, stratify=patient_class)\n",
    "\n",
    "# Using the patient subsets to select the audio samples\n",
    "audio_data_train = exp_metadata[(exp_metadata[PATIENT_ID_COLUMN].isin(patients_train))]\n",
    "audio_train = audio_data_train[AUDIO_ID_COLUMN]\n",
    "audio_label_train = audio_data_train[CLASS_COLUMN]\n",
    "\n",
    "audio_data_test = exp_metadata[(exp_metadata[PATIENT_ID_COLUMN].isin(patients_test))]\n",
    "audio_test = audio_data_test[AUDIO_ID_COLUMN]\n",
    "audio_label_test = audio_data_test[CLASS_COLUMN]\n",
    "\n",
    "# Print the final length of each subset\n",
    "print(f\"Test-set: {len(patients_test)} patients & {len(audio_data_test)} samples\")\n",
    "print(f\"Train-set: {len(patients_train):} patients & {len(audio_data_train)} samples\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-32-fc072d39b75b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    249\u001B[0m \u001B[0;31m#TODO: MAKE SCP FROM DATAFRAME\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    250\u001B[0m \u001B[0;31m# Get Feats\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 251\u001B[0;31m \u001B[0mtrain_feats\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_labels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmake_feats\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath_wav\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maudio_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maudio_label_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeature_config\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    252\u001B[0m \u001B[0mtest_feats\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_labels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmake_feats\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath_wav\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maudio_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maudio_label_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeature_config\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-32-fc072d39b75b>\u001B[0m in \u001B[0;36mmake_feats\u001B[0;34m(path_wav, audio_id, labels, feats_config)\u001B[0m\n\u001B[1;32m    229\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mid\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabel\u001B[0m \u001B[0;32min\u001B[0m \u001B[0maudio_id\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miteritems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    230\u001B[0m         \u001B[0;31m# Prepare features\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 231\u001B[0;31m         \u001B[0mF\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mFE\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextract\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath_wav\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mid\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'.wav'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    232\u001B[0m         \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    233\u001B[0m         \u001B[0megs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconcatenate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlabel\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: unsupported operand type(s) for +: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "import torchaudio\n",
    "import opensmile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Feature extractor\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Class for feature extraction\n",
    "    args: input arguments dictionary\n",
    "    Mandatory arguments: resampling_rate, feature_type, window_size, hop_length\n",
    "    For MFCC: f_max, n_mels, n_mfcc\n",
    "    For MelSpec/logMelSpec: f_max, n_mels\n",
    "    Optional arguments: compute_deltas, compute_delta_deltas\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args: dict):\n",
    "\n",
    "        self.args = args\n",
    "        self.resampling_rate = self.args['resampling_rate']\n",
    "        assert (args['feature_type'] in ['MFCC', 'MelSpec', 'logMelSpec', 'ComParE_2016_llds', 'ComParE_2016_voicing',\n",
    "                                         'ComParE_2016_spectral',\n",
    "                                         'ComParE_2016_mfcc', 'ComParE_2016_rasta', 'ComParE_2016_basic_spectral',\n",
    "                                         'ComParE_2016_energy'\n",
    "                                         ]), (\n",
    "            'Expected the feature_type to be MFCC / MelSpec / logMelSpec / ComParE_2016')\n",
    "\n",
    "        if self.args['feature_type'] == 'MFCC':\n",
    "            self.feature_transform = torchaudio.transforms.MFCC(sample_rate=self.resampling_rate,\n",
    "                                                                n_mfcc=int(self.args['n_mfcc']),\n",
    "                                                                melkwargs={\n",
    "                                                                    'n_fft': int(float(self.args[\n",
    "                                                                                           'window_size']) * 1e-3 * self.resampling_rate),\n",
    "                                                                    'n_mels': int(self.args['n_mels']),\n",
    "                                                                    'f_max': int(self.args['f_max']),\n",
    "                                                                    'hop_length': int(float(self.args[\n",
    "                                                                                                'hop_length']) * 1e-3 * self.resampling_rate)})\n",
    "        elif self.args['feature_type'] in ['MelSpec', 'logMelSpec']:\n",
    "            self.feature_transform = torchaudio.transforms.MelSpectrogram(sample_rate=self.resampling_rate,\n",
    "                                                                          n_fft=int(float(self.args[\n",
    "                                                                                              'window_size']) * 1e-3 * self.resampling_rate),\n",
    "                                                                          n_mels=int(self.args['n_mels']),\n",
    "                                                                          f_max=int(self.args['f_max']),\n",
    "                                                                          hop_length=int(float(self.args[\n",
    "                                                                                                   'hop_length']) * 1e-3 * self.resampling_rate))\n",
    "        elif 'ComParE_2016' in self.args['feature_type']:\n",
    "            self.feature_transform = opensmile.Smile(feature_set=opensmile.FeatureSet.ComParE_2016,\n",
    "                                                     feature_level=opensmile.FeatureLevel.LowLevelDescriptors,\n",
    "                                                     sampling_rate=self.resampling_rate)\n",
    "        else:\n",
    "            raise ValueError('Feature type not implemented')\n",
    "\n",
    "    def _read_audio(self, filepath):\n",
    "        \"\"\" This code does the following:\n",
    "                1. Read audio,\n",
    "                2. Resample the audio if required,\n",
    "                3. Perform waveform normalization,\n",
    "                4. Compute sound activity using threshold based method\n",
    "                5. Discard the silence regions\n",
    "        :param filepath: path to the audio file\n",
    "        :return: a torch.Tensor with the audio samples and an int with the sample rate\n",
    "        \"\"\"\n",
    "\n",
    "        s, fs = torchaudio.load(filepath)\n",
    "        if fs != self.resampling_rate:\n",
    "            s, fs = torchaudio.sox_effects.apply_effects_tensor(s, fs, [['rate', str(self.resampling_rate)]])\n",
    "        if s.shape[0] > 1:\n",
    "            s = s.mean(dim=0).unsqueeze(0)\n",
    "        s = s / torch.max(torch.abs(s))\n",
    "        sad = self.compute_sad(s.numpy(), self.resampling_rate)\n",
    "        s = s[np.where(sad == 1)]\n",
    "        return s, fs\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_sad(sig, fs, threshold=0.0001, sad_start_end_sil_length=100, sad_margin_length=50):\n",
    "        \"\"\" Compute threshold based sound activity \"\"\"\n",
    "        # Leading/Trailing margin\n",
    "        sad_start_end_sil_length = int(sad_start_end_sil_length * 1e-3 * fs)\n",
    "        # Margin around active samples\n",
    "        sad_margin_length = int(sad_margin_length * 1e-3 * fs)\n",
    "\n",
    "        sample_activity = np.zeros(sig.shape)\n",
    "        sample_activity[np.power(sig, 2) > threshold] = 1\n",
    "        sad = np.zeros(sig.shape)\n",
    "        for i in range(sample_activity.shape[1]):\n",
    "            if sample_activity[0, i] == 1:\n",
    "                sad[0, i - sad_margin_length:i + sad_margin_length] = 1\n",
    "        sad[0, 0:sad_start_end_sil_length] = 0\n",
    "        sad[0, -sad_start_end_sil_length:] = 0\n",
    "        return sad\n",
    "\n",
    "    def _do_feature_extraction(self, s):\n",
    "        \"\"\" Feature preparation\n",
    "        Steps:\n",
    "        1. Apply feature extraction to waveform\n",
    "        2. Convert amplitude to dB if required\n",
    "        3. Append delta and delta-delta features\n",
    "        \"\"\"\n",
    "\n",
    "        if 'ComParE_2016' in self.args['feature_type']:\n",
    "\n",
    "            # get a random string\n",
    "            file_name = ''.join(random.choices(string.ascii_uppercase + string.digits, k=4))\n",
    "            while os.path.exists(file_name):\n",
    "                file_name = ''.join(random.choices(string.ascii_uppercase + string.digits, k=4))\n",
    "            torchaudio.save(file_name + '.wav', s, sample_rate=self.resampling_rate)\n",
    "            F = self.feature_transform.process_file(file_name + '.wav')\n",
    "\n",
    "            # columns based selection\n",
    "            os.remove(file_name + '.wav')\n",
    "\n",
    "            # feature subsets\n",
    "            feature_subset = {}\n",
    "            if self.args['feature_type'] == 'ComParE_2016_voicing':\n",
    "                feature_subset['subset'] = ['F0final_sma', 'voicingFinalUnclipped_sma', 'jitterLocal_sma',\n",
    "                                            'jitterDDP_sma', 'shimmerLocal_sma', 'logHNR_sma']\n",
    "\n",
    "            if self.args['feature_type'] == 'ComParE_2016_energy':\n",
    "                feature_subset['subset'] = ['audspec_lengthL1norm_sma', 'audspecRasta_lengthL1norm_sma',\n",
    "                                            'pcm_RMSenergy_sma', 'pcm_zcr_sma']\n",
    "\n",
    "            if self.args['feature_type'] == 'ComParE_2016_spectral':\n",
    "                feature_subset['subset'] = ['audSpec_Rfilt_sma[0]', 'audSpec_Rfilt_sma[1]', 'audSpec_Rfilt_sma[2]',\n",
    "                                            'audSpec_Rfilt_sma[3]',\n",
    "                                            'audSpec_Rfilt_sma[4]', 'audSpec_Rfilt_sma[5]', 'audSpec_Rfilt_sma[6]',\n",
    "                                            'audSpec_Rfilt_sma[7]', 'audSpec_Rfilt_sma[8]', 'audSpec_Rfilt_sma[9]',\n",
    "                                            'audSpec_Rfilt_sma[10]', 'audSpec_Rfilt_sma[11]', 'audSpec_Rfilt_sma[12]',\n",
    "                                            'audSpec_Rfilt_sma[13]',\n",
    "                                            'audSpec_Rfilt_sma[14]', 'audSpec_Rfilt_sma[15]', 'audSpec_Rfilt_sma[16]',\n",
    "                                            'audSpec_Rfilt_sma[17]',\n",
    "                                            'audSpec_Rfilt_sma[18]', 'audSpec_Rfilt_sma[19]', 'audSpec_Rfilt_sma[20]',\n",
    "                                            'audSpec_Rfilt_sma[21]',\n",
    "                                            'audSpec_Rfilt_sma[22]', 'audSpec_Rfilt_sma[23]', 'audSpec_Rfilt_sma[24]',\n",
    "                                            'audSpec_Rfilt_sma[25]',\n",
    "                                            'pcm_fftMag_fband250-650_sma', 'pcm_fftMag_fband1000-4000_sma',\n",
    "                                            'pcm_fftMag_spectralRollOff25.0_sma',\n",
    "                                            'pcm_fftMag_spectralRollOff50.0_sma', 'pcm_fftMag_spectralRollOff75.0_sma',\n",
    "                                            'pcm_fftMag_spectralRollOff90.0_sma', 'pcm_fftMag_spectralFlux_sma',\n",
    "                                            'pcm_fftMag_spectralCentroid_sma', 'pcm_fftMag_spectralEntropy_sma',\n",
    "                                            'pcm_fftMag_spectralVariance_sma', 'pcm_fftMag_spectralSkewness_sma',\n",
    "                                            'pcm_fftMag_spectralKurtosis_sma', 'pcm_fftMag_spectralSlope_sma',\n",
    "                                            'pcm_fftMag_psySharpness_sma', 'pcm_fftMag_spectralHarmonicity_sma',\n",
    "                                            'mfcc_sma[1]', 'mfcc_sma[2]', 'mfcc_sma[3]', 'mfcc_sma[4]', 'mfcc_sma[5]',\n",
    "                                            'mfcc_sma[6]', 'mfcc_sma[7]', 'mfcc_sma[8]',\n",
    "                                            'mfcc_sma[9]', 'mfcc_sma[10]', 'mfcc_sma[11]', 'mfcc_sma[12]',\n",
    "                                            'mfcc_sma[13]', 'mfcc_sma[14]']\n",
    "\n",
    "            if self.args['feature_type'] == 'ComParE_2016_mfcc':\n",
    "                feature_subset['subset'] = ['mfcc_sma[1]', 'mfcc_sma[2]', 'mfcc_sma[3]', 'mfcc_sma[4]', 'mfcc_sma[5]',\n",
    "                                            'mfcc_sma[6]', 'mfcc_sma[7]', 'mfcc_sma[8]',\n",
    "                                            'mfcc_sma[9]', 'mfcc_sma[10]', 'mfcc_sma[11]', 'mfcc_sma[12]',\n",
    "                                            'mfcc_sma[13]', 'mfcc_sma[14]']\n",
    "\n",
    "            if self.args['feature_type'] == 'ComParE_2016_rasta':\n",
    "                feature_subset['subset'] = ['audSpec_Rfilt_sma[0]', 'audSpec_Rfilt_sma[1]', 'audSpec_Rfilt_sma[2]',\n",
    "                                            'audSpec_Rfilt_sma[3]',\n",
    "                                            'audSpec_Rfilt_sma[4]', 'audSpec_Rfilt_sma[5]', 'audSpec_Rfilt_sma[6]',\n",
    "                                            'audSpec_Rfilt_sma[7]', 'audSpec_Rfilt_sma[8]', 'audSpec_Rfilt_sma[9]',\n",
    "                                            'audSpec_Rfilt_sma[10]', 'audSpec_Rfilt_sma[11]', 'audSpec_Rfilt_sma[12]',\n",
    "                                            'audSpec_Rfilt_sma[13]',\n",
    "                                            'audSpec_Rfilt_sma[14]', 'audSpec_Rfilt_sma[15]', 'audSpec_Rfilt_sma[16]',\n",
    "                                            'audSpec_Rfilt_sma[17]',\n",
    "                                            'audSpec_Rfilt_sma[18]', 'audSpec_Rfilt_sma[19]', 'audSpec_Rfilt_sma[20]',\n",
    "                                            'audSpec_Rfilt_sma[21]',\n",
    "                                            'audSpec_Rfilt_sma[22]', 'audSpec_Rfilt_sma[23]', 'audSpec_Rfilt_sma[24]',\n",
    "                                            'audSpec_Rfilt_sma[25]']\n",
    "\n",
    "            if self.args['feature_type'] == 'ComParE_2016_basic_spectral':\n",
    "                feature_subset['subset'] = ['pcm_fftMag_fband250-650_sma', 'pcm_fftMag_fband1000-4000_sma',\n",
    "                                            'pcm_fftMag_spectralRollOff25.0_sma',\n",
    "                                            'pcm_fftMag_spectralRollOff50.0_sma', 'pcm_fftMag_spectralRollOff75.0_sma',\n",
    "                                            'pcm_fftMag_spectralRollOff90.0_sma', 'pcm_fftMag_spectralFlux_sma',\n",
    "                                            'pcm_fftMag_spectralCentroid_sma', 'pcm_fftMag_spectralEntropy_sma',\n",
    "                                            'pcm_fftMag_spectralVariance_sma', 'pcm_fftMag_spectralSkewness_sma',\n",
    "                                            'pcm_fftMag_spectralKurtosis_sma', 'pcm_fftMag_spectralSlope_sma',\n",
    "                                            'pcm_fftMag_psySharpness_sma', 'pcm_fftMag_spectralHarmonicity_sma']\n",
    "\n",
    "            if self.args['feature_type'] == 'ComParE_2016_llds':\n",
    "                feature_subset['subset'] = list(F.columns)\n",
    "\n",
    "            F = F[feature_subset['subset']].to_numpy()\n",
    "            F = np.nan_to_num(F)\n",
    "            F = torch.from_numpy(F).T\n",
    "\n",
    "        if self.args['feature_type'] == 'MelSpec':\n",
    "            F = self.feature_transform(s)\n",
    "\n",
    "        if self.args['feature_type'] == 'logMelSpec':\n",
    "            F = self.feature_transform(s)\n",
    "            F = torchaudio.functional.amplitude_to_DB(F, multiplier=10, amin=1e-10, db_multiplier=0)\n",
    "\n",
    "        if self.args['feature_type'] == 'MFCC':\n",
    "            F = self.feature_transform(s)\n",
    "\n",
    "        if self.args.get('compute_deltas', False):\n",
    "            FD = torchaudio.functional.compute_deltas(F)\n",
    "            F = torch.cat((F, FD), dim=0)\n",
    "\n",
    "        if self.args.get('compute_delta_deltas', False):\n",
    "            FDD = torchaudio.functional.compute_deltas(FD)\n",
    "            F = torch.cat((F, FDD), dim=0)\n",
    "        return F.T\n",
    "\n",
    "    def extract(self, filepath):\n",
    "        ''' Interface to other codes for this class\n",
    "\t\tSteps:\n",
    "\t\t1. Read audio\n",
    "\t\t2. Do feature extraction\n",
    "\t\t'''\n",
    "        self.audio_path = filepath\n",
    "        s, fs = self._read_audio(filepath)\n",
    "        return self._do_feature_extraction(s)\n",
    "\n",
    "# Feature Extractor\n",
    "def make_feats(path_wav: str, audio_id: pd.DataFrame, labels: pd.DataFrame,feats_config: dict):\n",
    "    \"\"\"\n",
    "    Extract features from audio files\n",
    "    :param path_wav: path to audio files\n",
    "    :param audio_id: audio ids as a pandas dataframe\n",
    "    :param labels: labels as a pandas dataframe\n",
    "    :param feats_config: feature configuration as a dictionary\n",
    "    \"\"\"\n",
    "    # Prepare feature extractor\n",
    "    FE = FeatureExtractor(feats_config)\n",
    "\n",
    "    egs = []\n",
    "    for id, label in audio_id.iteritems():\n",
    "        # Prepare features\n",
    "        F = FE.extract(os.path.join(path_wav, id + '.wav'))\n",
    "        np.array(F)\n",
    "        egs.append(np.concatenate((np.array(F), np.array([label] * F.shape[0]).reshape(F.shape[0], 1)), axis=1))\n",
    "    egs = np.vstack(egs)\n",
    "\n",
    "    return np.array(egs[:, :-1], dtype=float), np.array(egs[:, -1], dtype=int)\n",
    "\n",
    "# Feature configuration\n",
    "feature_config = {'feature_type': 'MFCC',\n",
    "                  'resampling_rate': 44100,\n",
    "                  'n_mels': 64,\n",
    "                  'n_mfcc': 13,\n",
    "                  'f_max': 22050,\n",
    "                  'window_size': 25,\n",
    "                  'hop_length': 10,\n",
    "                  'compute_deltas': True,\n",
    "                  'compute_delta_deltas': True}\n",
    "\n",
    "# Get Feats\n",
    "train_feats, train_labels = make_feats(path_wav, audio_train, audio_label_train, feature_config)\n",
    "test_feats, test_labels = make_feats(path_wav, audio_test, audio_label_test, feature_config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Torch utils\n",
    "def activations(act):\n",
    "    \"\"\"\n",
    "    Interface to fetch activations\n",
    "    \"\"\"\n",
    "    activ = {'Tanh': nn.Tanh(), 'ReLU': nn.ReLU(), 'Sigmoid': nn.Sigmoid()}\n",
    "    act = activ[act]\n",
    "\n",
    "    if act is not None:\n",
    "        return act\n",
    "    else:\n",
    "        raise ValueError('Unknown activation, add it in activations dictionary in models.py')\n",
    "\n",
    "class bce_loss(nn.Module):\n",
    "    \"\"\"\n",
    "    Class interface to compute BCE loss\n",
    "    Default uses mean reduction equal weight for both positive and negative samples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reduction='mean', pos_weight=torch.tensor([1])):\n",
    "        super(bce_loss, self).__init__()\n",
    "        self.criterion = nn.BCEWithLogitsLoss(reduction=reduction, pos_weight=pos_weight)\n",
    "\n",
    "    def forward(self, net_out, ref):\n",
    "        return self.criterion(net_out, ref)\n",
    "\n",
    "class FFClassificationHead(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(FFClassificationHead, self).__init__()\n",
    "\n",
    "        self.inDim = args['input_dimension']\n",
    "        self.units = [self.inDim] + [item for item in args['units'] if item > 0]\n",
    "        self.num_layers = len(self.units) - 1\n",
    "\n",
    "        self.activation_type = args['activation']\n",
    "        self.dropout_p = args['dropout']\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            setattr(self, 'linearlayer_' + str(i), nn.Linear(self.units[i], self.units[i + 1]))\n",
    "            setattr(self, 'dropout_' + str(i), nn.Dropout(self.dropout_p))\n",
    "        self.linearOut = nn.Linear(self.units[-1], 1)\n",
    "        self.activation = activations(self.activation_type)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        x = torch.vstack(inputs)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = getattr(self, 'linearlayer_' + str(i))(x)\n",
    "            x = self.activation(x)\n",
    "            x = getattr(self, 'dropout_' + str(i))(x)\n",
    "        x = self.linearOut(x)\n",
    "        return [x[i, ] for i in range(x.shape[0])]\n",
    "\n",
    "# LSTM ENCODER classifier\n",
    "class LSTMEncoder(nn.Module):\n",
    "    \"\"\" Stacked (B)LSTM Encoder\n",
    "    Arguments:\n",
    "    args: Dictionary with below entries\n",
    "    input_dimenstion: (integer), Dimension of the feature vector input\n",
    "    units: (integer), Number of LSTM units. Default: 128\n",
    "    num_layers: (integer), Number of layers in the stacked LSTM. Default: 2\n",
    "    bidirectional: (bool), if True biLSTM will be used. Default: True\n",
    "    apply_mean_norm: (bool), subtract the example level mean. Default: False\n",
    "    apply_var_norm: (bool), normalize by standard deviation. Default: False\n",
    "    pooltype: (['average' or 'last']). Default: 'average'\n",
    "    ----> 'average': average of the LSTM output along time dimension is the embedding\n",
    "    ----> 'last': LSTM hidden state at the last time-step of the last layer is the embedding\n",
    "    dropout: (float), Dropout probability. Default: 0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.inDim = args['input_dimension']\n",
    "        self.units = args.get('units', 128)\n",
    "        self.num_layers = args.get('num_layers', 2)\n",
    "        self.bidirectional = args.get('bidirectional', False)\n",
    "\n",
    "        self.apply_mean_norm = args.get('apply_mean_norm', False)\n",
    "        self.apply_var_norm = args.get('apply_var_norm', False)\n",
    "        self.dropout_p = args.get('dropout', 0)\n",
    "        assert self.dropout_p < 1\n",
    "\n",
    "        self.pooltype = args.get('pooltype', False)\n",
    "        assert self.pooltype in ['average', 'last']\n",
    "\n",
    "        self.LSTM = nn.LSTM(self.inDim,\n",
    "                            self.units,\n",
    "                            num_layers=self.num_layers,\n",
    "                            bidirectional=self.bidirectional,\n",
    "                            batch_first=True,\n",
    "                            dropout=self.dropout_p)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: a list of torch tensors\n",
    "        The tensors can be of varying length.\n",
    "        \"\"\"\n",
    "        inlens = [x.shape[0] for x in inputs]\n",
    "        if self.apply_mean_norm:\n",
    "            inputs = [F - torch.mean(F, dim=0) for F in inputs]\n",
    "        if self.apply_var_norm:\n",
    "            inputs = [F / torch.std(F, dim=0) for F in inputs]\n",
    "\n",
    "        x = pad_sequence(inputs, batch_first=True)\n",
    "        x = pack_padded_sequence(x, inlens, batch_first=True, enforce_sorted=False)\n",
    "        x, hc = self.LSTM(x)\n",
    "\n",
    "        if self.pooltype == 'average':\n",
    "            x, _ = pad_packed_sequence(x, batch_first=True)\n",
    "            x = torch.sum(x, dim=1)\n",
    "            x = torch.div(x, torch.tensor(inlens).unsqueeze(1).repeat(1, x.shape[1]).to(x.device))\n",
    "        elif self.pooltype == 'last':\n",
    "            if self.bidirectional:\n",
    "                x = hc[0][-2:, :, :].transpose(0, 1).reshape(hc[0].shape[1], 2 * hc[0].shape[2])\n",
    "            else:\n",
    "                x = hc[0][-1, :, :]\n",
    "        else:\n",
    "            raise ValueError('Unknown pooling method')\n",
    "\n",
    "        return [x[i, :].view(1, x.shape[1]) for i in range(x.shape[0])]\n",
    "\n",
    "# LSTM classifier\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Classifier architecture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        self.input_dimension = args['input_dimension']\n",
    "        self.lstm_encoder_units = args['lstm_encoder_units']\n",
    "        self.lstm_num_layers = args['lstm_num_layers']\n",
    "        self.lstm_bidirectional = args['lstm_bidirectional']\n",
    "        self.lstm_dropout_p = args['lstm_dropout']\n",
    "        self.lstm_pooling = args['lstm_pooling']\n",
    "        self.apply_mean_norm = args['apply_mean_norm']\n",
    "        self.apply_var_norm = args['apply_var_norm']\n",
    "\n",
    "        encoder_args = {'input_dimension': self.input_dimension, 'units': self.lstm_encoder_units,\n",
    "                        'num_layers': self.lstm_num_layers, 'bidirectional': self.lstm_bidirectional,\n",
    "                        'apply_mean_norm': self.apply_mean_norm, 'apply_var_norm': self.apply_var_norm,\n",
    "                        'dropout': self.lstm_dropout_p, 'pooltype': self.lstm_pooling}\n",
    "\n",
    "        self.encoder = LSTMEncoder(encoder_args)\n",
    "\n",
    "        temp = args['classifier_units']\n",
    "        if type(temp) == list:\n",
    "            self.classifier_units = temp\n",
    "        else:\n",
    "            self.classifier_units = [temp]\n",
    "        self.classifier_activation = args['classifier_activation']\n",
    "        self.classifier_dropout_p = args['classifier_dropout']\n",
    "        cls_idim = 2 * self.lstm_encoder_units if self.lstm_bidirectional else self.lstm_encoder_units\n",
    "        classifier_args = {'input_dimension': cls_idim, 'units': self.classifier_units,\n",
    "                           'dropout': self.classifier_dropout_p, 'activation': self.classifier_activation}\n",
    "\n",
    "        self.classifier = FFClassificationHead(classifier_args)\n",
    "        self.criterion = bce_loss()\n",
    "\n",
    "    def init_encoder(self, params):\n",
    "        \"\"\"\n",
    "        Initialize the feature encoder using a pre-trained model\n",
    "        \"\"\"\n",
    "        self.encoder.load_state_dict(params)\n",
    "\n",
    "    def init_classifier(self, params):\n",
    "        \"\"\"\n",
    "        Initialize the classification-head using a pre-trained classifier model\n",
    "        \"\"\"\n",
    "        self.classifier.load_state_dict(params)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\"\n",
    "        Prediction of the classifier score\n",
    "        \"\"\"\n",
    "        return self.classifier(self.encoder(inputs))\n",
    "\n",
    "    def predict_proba(self, inputs):\n",
    "        \"\"\"\n",
    "        Prediction of the posterior probability\n",
    "        \"\"\"\n",
    "        return [torch.sigmoid(item) for item in self.predict(inputs)]\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Forward pass through the network and loss computation\n",
    "        \"\"\"\n",
    "        return self.criterion(torch.stack(self.predict(inputs)), torch.stack(targets))\n",
    "\n",
    "# Models configurations\n",
    "ALL_MODELS = {'LogisticRegression': {'c': 0.01,\n",
    "                                     'max_iter': 40,\n",
    "                                     'solver': 'liblinear',\n",
    "                                     'penalty': 'l2',\n",
    "                                     'class_weight': 'balanced', 'random_state': SEED, 'verbose': True},\n",
    "              'RandomForest': {'n_estimators': 20,\n",
    "                               'criterion': 'gini',\n",
    "                               'max_depth': None,\n",
    "                               'min_samples_split': 2,\n",
    "                               'min_samples_leaf': 1,\n",
    "                               'max_features': 'sqrt',\n",
    "                               'class_weight': 'balanced', 'random_state': SEED, 'verbose': True},\n",
    "              'MLP': {'learning_rate_init': 0.001, 'alpha': 0.001, 'solver': 'adam', 'hidden_layer_sizes': [20, 20], 'max_iter': 500, 'activation': 'tanh',\n",
    "                      'class_weight': 'balanced', 'random_state': SEED, 'verbose': True},\n",
    "              'linearSVM': {'penalty': 'l2',\n",
    "                            'loss': 'squared_hinge',\n",
    "                            'c': 0.01,\n",
    "                            'tol': 1e-4,\n",
    "                            'max_iter': 100,\n",
    "                            'class_weight': 'balanced', 'random_state': SEED, 'verbose': True}\n",
    "              }\n",
    "\n",
    "# Training function\n",
    "def config_model(model_name, training_feats, training_labels):\n",
    "\n",
    "    model_args = ALL_MODELS[model_exp]\n",
    "\n",
    "    if model_name == 'LogisticRegression':\n",
    "        model = LogisticRegression(C=float(model_args['c']),\n",
    "                                   max_iter=int(model_args['max_iter']),\n",
    "                                   solver=model_args['solver'],\n",
    "                                   penalty=model_args['penalty'],\n",
    "                                   class_weight=model_args['class_weight'],\n",
    "                                   random_state=model_args['random_state'],\n",
    "                                   verbose=True)\n",
    "\n",
    "    elif model_name == 'RandomForest':\n",
    "        model = RandomForestClassifier(n_estimators=model_args['n_estimators'],\n",
    "                                       criterion=model_args['criterion'],\n",
    "                                       max_depth=model_args['max_depth'],\n",
    "                                       min_samples_split=model_args['min_samples_split'],\n",
    "                                       min_samples_leaf=model_args['min_samples_leaf'],\n",
    "                                       max_features=model_args['max_features'],\n",
    "                                       class_weight=model_args['class_weight'],\n",
    "                                       random_state=model_args['random_state'])\n",
    "\n",
    "    elif model_name == 'LinearSVM':\n",
    "        model = SVC(penalty=model_args['penalty'],\n",
    "                    loss=model_args['loss'],\n",
    "                    C=model_args['c'],\n",
    "                    tol=model_args['tol'],\n",
    "                    max_iter = model_args['max_iter'],\n",
    "                    verbose=model_args['verbose'],\n",
    "                    class_weight=model_args['class_weight'],\n",
    "                    random_state=model_args['random_state'])\n",
    "\n",
    "    elif model_name == 'MLP':\n",
    "        model = MLPClassifier(hidden_layer_sizes=model_args['hidden_layer_sizes'],\n",
    "                              solver=model_args['solver'], alpha=model_args['alpha'],\n",
    "                              learning_rate_init=model_args['learning_rate_init'],\n",
    "                              verbose=model_args['verbose'], activation=model_args['activation'],\n",
    "                              max_iter=model_args['max_iter'], random_state=model_args['random_state'])\n",
    "\n",
    "        if model_args['class_weight'] == 'balanced':\n",
    "            train_data = np.concatenate((training_feats, training_labels.reshape(training_feats.shape[0], 1)), axis=1)\n",
    "            ind = np.where(train_data[:, -1] == 1)[0]\n",
    "            n_positives = len(ind)\n",
    "            n_negatives = train_data.shape[0] - n_positives\n",
    "            up_sample_factor = int(n_negatives / n_positives) - 1\n",
    "            for i in range(up_sample_factor):\n",
    "                train_data = np.concatenate((train_data, train_data[ind, :]), axis=0)\n",
    "            np.random.shuffle(train_data)\n",
    "            training_feats = train_data[:, :-1]\n",
    "            training_labels = train_data[:, -1]\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Not implementation of the model: \" + model_exp)\n",
    "    return model, training_feats, training_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}