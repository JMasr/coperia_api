{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook of COPERIA UVigo-GTM team\n",
    "# @autor: José M. Ramírez @email: jmramirez@gts.uvigo.es @date: 2023-02-27 @version: 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "import torchaudio\n",
    "import opensmile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "# Define important paths\n",
    "path_data = '/home/jsanhcez/Documentos/Proyectos/99_to_do_COPERIA/repos/coperia_api/dataset_dicoperia/'\n",
    "path_feats = 'data/features/'\n",
    "path_results = 'results/'\n",
    "path_models = 'models/'\n",
    "path_notebooks = '/home/jsanhcez/Documentos/Proyectos/99_to_do_COPERIA/repos/coperia_api/notebooks'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "# Define the data to be used\n",
    "CLASSES = ['covid-control', 'covid-persistente']\n",
    "\n",
    "dicoperia_metadata = pd.read_csv(os.path.join(path_data, 'metadata_dicoperia.csv'), decimal=',')\n",
    "dicoperia_filters = {'audio_id': ['c15e54fc-5290-4652-a3f7-ff3b779bd980', '244b61cc-4fd7-4073-b0d8-7bacd42f6202'],\n",
    "                     'patient_type': ['coperia-rehab'],\n",
    "                     'audio_type': ['/a/'],\n",
    "                     'audio_moment': ['after']}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering the metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 747.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering DONE!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def make_dicoperia_metadata(root_path:str, metadata: pd.DataFrame, filter: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Make a metadata file for the COPERIA dataset filtering some columns\n",
    "    :param root_path: root path of the data directory\n",
    "    :param metadata: a list with all the audio samples in COPERIA as an Audio class\n",
    "    :param filter: a dictionary with the columns and values to filter\n",
    "    :return: a pandas dataframe with the metadata of the DICOPERIA dataset\n",
    "    \"\"\"\n",
    "    print('Filtering the metadata...')\n",
    "\n",
    "    df = metadata.copy()\n",
    "    for key, values in tqdm(filter.items()):\n",
    "        df = df[~df[key].isin(values)]\n",
    "    df.to_csv(os.path.join(root_path, 'metadata_dicoperia.csv'), index=False, decimal=',')\n",
    "    print('Filtering DONE!!')\n",
    "    df.replace(CLASSES, [0, 1], inplace=True)\n",
    "    return df\n",
    "\n",
    "exp_metadata = make_dicoperia_metadata(path_notebooks, dicoperia_metadata, dicoperia_filters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "# Global seed\n",
    "SEED = 42\n",
    "# Define the k-folds\n",
    "K_NUM = 5\n",
    "SAMPLE_GAIN_TESTING = 0.05\n",
    "# Define the interest dataset columns\n",
    "AUDIO_ID_COLUMN = 'audio_id'\n",
    "PATIENT_ID_COLUMN = 'patient_id'\n",
    "\n",
    "CLASS_COLUMN = 'patient_type'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set: 7 patients & 21 samples\n",
      "Train-set: 131 patients & 396 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Making the subsets by patients\n",
    "patient_data = exp_metadata[[PATIENT_ID_COLUMN, CLASS_COLUMN]].drop_duplicates()\n",
    "patient_id = patient_data[PATIENT_ID_COLUMN]\n",
    "patient_class = patient_data[CLASS_COLUMN]\n",
    "\n",
    "patients_train, patients_test, patient_labels_train, patient_labels_test = train_test_split(patient_id, patient_class, test_size=SAMPLE_GAIN_TESTING, random_state=SEED, stratify=patient_class)\n",
    "\n",
    "# Using the patient subsets to select the audio samples\n",
    "audio_data_train = exp_metadata[(exp_metadata[PATIENT_ID_COLUMN].isin(patients_train))]\n",
    "audio_train = audio_data_train[AUDIO_ID_COLUMN]\n",
    "audio_label_train = audio_data_train[CLASS_COLUMN]\n",
    "\n",
    "audio_data_test = exp_metadata[(exp_metadata[PATIENT_ID_COLUMN].isin(patients_test))]\n",
    "audio_test = audio_data_test[AUDIO_ID_COLUMN]\n",
    "audio_label_test = audio_data_test[CLASS_COLUMN]\n",
    "\n",
    "# Print the final length of each subset\n",
    "print(f\"Test-set: {len(patients_test)} patients & {len(audio_data_test)} samples\")\n",
    "print(f\"Train-set: {len(patients_train):} patients & {len(audio_data_train)} samples\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "# Feature extractor\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Class for feature extraction\n",
    "    args: input arguments dictionary\n",
    "    Mandatory arguments: resampling_rate, feature_type, window_size, hop_length\n",
    "    For MFCC: f_max, n_mels, n_mfcc\n",
    "    For MelSpec/logMelSpec: f_max, n_mels\n",
    "    Optional arguments: compute_deltas, compute_delta_deltas\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args: dict):\n",
    "\n",
    "        self.args = args\n",
    "        self.resampling_rate = self.args['resampling_rate']\n",
    "        assert (args['feature_type'] in ['MFCC', 'MelSpec', 'logMelSpec', 'ComParE_2016_llds', 'ComParE_2016_voicing',\n",
    "                                         'ComParE_2016_spectral',\n",
    "                                         'ComParE_2016_mfcc', 'ComParE_2016_rasta', 'ComParE_2016_basic_spectral',\n",
    "                                         'ComParE_2016_energy'\n",
    "                                         ]), (\n",
    "            'Expected the feature_type to be MFCC / MelSpec / logMelSpec / ComParE_2016')\n",
    "\n",
    "        if self.args['feature_type'] == 'MFCC':\n",
    "            self.feature_transform = torchaudio.transforms.MFCC(sample_rate=self.resampling_rate,\n",
    "                                                                n_mfcc=int(self.args['n_mfcc']),\n",
    "                                                                melkwargs={\n",
    "                                                                    'n_fft': int(float(self.args[\n",
    "                                                                                           'window_size']) * 1e-3 * self.resampling_rate),\n",
    "                                                                    'n_mels': int(self.args['n_mels']),\n",
    "                                                                    'f_max': int(self.args['f_max']),\n",
    "                                                                    'hop_length': int(float(self.args[\n",
    "                                                                                                'hop_length']) * 1e-3 * self.resampling_rate)})\n",
    "        elif self.args['feature_type'] in ['MelSpec', 'logMelSpec']:\n",
    "            self.feature_transform = torchaudio.transforms.MelSpectrogram(sample_rate=self.resampling_rate,\n",
    "                                                                          n_fft=int(float(self.args[\n",
    "                                                                                              'window_size']) * 1e-3 * self.resampling_rate),\n",
    "                                                                          n_mels=int(self.args['n_mels']),\n",
    "                                                                          f_max=int(self.args['f_max']),\n",
    "                                                                          hop_length=int(float(self.args[\n",
    "                                                                                                   'hop_length']) * 1e-3 * self.resampling_rate))\n",
    "        elif 'ComParE_2016' in self.args['feature_type']:\n",
    "            self.feature_transform = opensmile.Smile(feature_set=opensmile.FeatureSet.ComParE_2016,\n",
    "                                                     feature_level=opensmile.FeatureLevel.LowLevelDescriptors,\n",
    "                                                     sampling_rate=self.resampling_rate)\n",
    "        else:\n",
    "            raise ValueError('Feature type not implemented')\n",
    "\n",
    "    def _read_audio(self, filepath):\n",
    "        \"\"\" This code does the following:\n",
    "                1. Read audio,\n",
    "                2. Resample the audio if required,\n",
    "                3. Perform waveform normalization,\n",
    "                4. Compute sound activity using threshold based method\n",
    "                5. Discard the silence regions\n",
    "        :param filepath: path to the audio file\n",
    "        :return: a torch.Tensor with the audio samples and an int with the sample rate\n",
    "        \"\"\"\n",
    "\n",
    "        s, fs = torchaudio.load(filepath)\n",
    "        if fs != self.resampling_rate:\n",
    "            s, fs = torchaudio.sox_effects.apply_effects_tensor(s, fs, [['rate', str(self.resampling_rate)]])\n",
    "        if s.shape[0] > 1:\n",
    "            s = s.mean(dim=0).unsqueeze(0)\n",
    "        s = s / torch.max(torch.abs(s))\n",
    "        sad = self.compute_sad(s.numpy(), self.resampling_rate)\n",
    "        s = s[np.where(sad == 1)]\n",
    "        return s, fs\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_sad(sig, fs, threshold=0.0001, sad_start_end_sil_length=100, sad_margin_length=50):\n",
    "        \"\"\" Compute threshold based sound activity \"\"\"\n",
    "        # Leading/Trailing margin\n",
    "        sad_start_end_sil_length = int(sad_start_end_sil_length * 1e-3 * fs)\n",
    "        # Margin around active samples\n",
    "        sad_margin_length = int(sad_margin_length * 1e-3 * fs)\n",
    "\n",
    "        sample_activity = np.zeros(sig.shape)\n",
    "        sample_activity[np.power(sig, 2) > threshold] = 1\n",
    "        sad = np.zeros(sig.shape)\n",
    "        for i in range(sample_activity.shape[1]):\n",
    "            if sample_activity[0, i] == 1:\n",
    "                sad[0, i - sad_margin_length:i + sad_margin_length] = 1\n",
    "        sad[0, 0:sad_start_end_sil_length] = 0\n",
    "        sad[0, -sad_start_end_sil_length:] = 0\n",
    "        return sad\n",
    "\n",
    "    def _do_feature_extraction(self, s):\n",
    "        \"\"\" Feature preparation\n",
    "        Steps:\n",
    "        1. Apply feature extraction to waveform\n",
    "        2. Convert amplitude to dB if required\n",
    "        3. Append delta and delta-delta features\n",
    "        \"\"\"\n",
    "\n",
    "        if 'ComParE_2016' in self.args['feature_type']:\n",
    "\n",
    "            # get a random string\n",
    "            file_name = ''.join(random.choices(string.ascii_uppercase + string.digits, k=4))\n",
    "            while os.path.exists(file_name):\n",
    "                file_name = ''.join(random.choices(string.ascii_uppercase + string.digits, k=4))\n",
    "            torchaudio.save(file_name + '.wav', s, sample_rate=self.resampling_rate)\n",
    "            F = self.feature_transform.process_file(file_name + '.wav')\n",
    "\n",
    "            # columns based selection\n",
    "            os.remove(file_name + '.wav')\n",
    "\n",
    "            # feature subsets\n",
    "            feature_subset = {}\n",
    "            if self.args['feature_type'] == 'ComParE_2016_voicing':\n",
    "                feature_subset['subset'] = ['F0final_sma', 'voicingFinalUnclipped_sma', 'jitterLocal_sma',\n",
    "                                            'jitterDDP_sma', 'shimmerLocal_sma', 'logHNR_sma']\n",
    "\n",
    "            if self.args['feature_type'] == 'ComParE_2016_energy':\n",
    "                feature_subset['subset'] = ['audspec_lengthL1norm_sma', 'audspecRasta_lengthL1norm_sma',\n",
    "                                            'pcm_RMSenergy_sma', 'pcm_zcr_sma']\n",
    "\n",
    "            if self.args['feature_type'] == 'ComParE_2016_spectral':\n",
    "                feature_subset['subset'] = ['audSpec_Rfilt_sma[0]', 'audSpec_Rfilt_sma[1]', 'audSpec_Rfilt_sma[2]',\n",
    "                                            'audSpec_Rfilt_sma[3]',\n",
    "                                            'audSpec_Rfilt_sma[4]', 'audSpec_Rfilt_sma[5]', 'audSpec_Rfilt_sma[6]',\n",
    "                                            'audSpec_Rfilt_sma[7]', 'audSpec_Rfilt_sma[8]', 'audSpec_Rfilt_sma[9]',\n",
    "                                            'audSpec_Rfilt_sma[10]', 'audSpec_Rfilt_sma[11]', 'audSpec_Rfilt_sma[12]',\n",
    "                                            'audSpec_Rfilt_sma[13]',\n",
    "                                            'audSpec_Rfilt_sma[14]', 'audSpec_Rfilt_sma[15]', 'audSpec_Rfilt_sma[16]',\n",
    "                                            'audSpec_Rfilt_sma[17]',\n",
    "                                            'audSpec_Rfilt_sma[18]', 'audSpec_Rfilt_sma[19]', 'audSpec_Rfilt_sma[20]',\n",
    "                                            'audSpec_Rfilt_sma[21]',\n",
    "                                            'audSpec_Rfilt_sma[22]', 'audSpec_Rfilt_sma[23]', 'audSpec_Rfilt_sma[24]',\n",
    "                                            'audSpec_Rfilt_sma[25]',\n",
    "                                            'pcm_fftMag_fband250-650_sma', 'pcm_fftMag_fband1000-4000_sma',\n",
    "                                            'pcm_fftMag_spectralRollOff25.0_sma',\n",
    "                                            'pcm_fftMag_spectralRollOff50.0_sma', 'pcm_fftMag_spectralRollOff75.0_sma',\n",
    "                                            'pcm_fftMag_spectralRollOff90.0_sma', 'pcm_fftMag_spectralFlux_sma',\n",
    "                                            'pcm_fftMag_spectralCentroid_sma', 'pcm_fftMag_spectralEntropy_sma',\n",
    "                                            'pcm_fftMag_spectralVariance_sma', 'pcm_fftMag_spectralSkewness_sma',\n",
    "                                            'pcm_fftMag_spectralKurtosis_sma', 'pcm_fftMag_spectralSlope_sma',\n",
    "                                            'pcm_fftMag_psySharpness_sma', 'pcm_fftMag_spectralHarmonicity_sma',\n",
    "                                            'mfcc_sma[1]', 'mfcc_sma[2]', 'mfcc_sma[3]', 'mfcc_sma[4]', 'mfcc_sma[5]',\n",
    "                                            'mfcc_sma[6]', 'mfcc_sma[7]', 'mfcc_sma[8]',\n",
    "                                            'mfcc_sma[9]', 'mfcc_sma[10]', 'mfcc_sma[11]', 'mfcc_sma[12]',\n",
    "                                            'mfcc_sma[13]', 'mfcc_sma[14]']\n",
    "\n",
    "            if self.args['feature_type'] == 'ComParE_2016_mfcc':\n",
    "                feature_subset['subset'] = ['mfcc_sma[1]', 'mfcc_sma[2]', 'mfcc_sma[3]', 'mfcc_sma[4]', 'mfcc_sma[5]',\n",
    "                                            'mfcc_sma[6]', 'mfcc_sma[7]', 'mfcc_sma[8]',\n",
    "                                            'mfcc_sma[9]', 'mfcc_sma[10]', 'mfcc_sma[11]', 'mfcc_sma[12]',\n",
    "                                            'mfcc_sma[13]', 'mfcc_sma[14]']\n",
    "\n",
    "            if self.args['feature_type'] == 'ComParE_2016_rasta':\n",
    "                feature_subset['subset'] = ['audSpec_Rfilt_sma[0]', 'audSpec_Rfilt_sma[1]', 'audSpec_Rfilt_sma[2]',\n",
    "                                            'audSpec_Rfilt_sma[3]',\n",
    "                                            'audSpec_Rfilt_sma[4]', 'audSpec_Rfilt_sma[5]', 'audSpec_Rfilt_sma[6]',\n",
    "                                            'audSpec_Rfilt_sma[7]', 'audSpec_Rfilt_sma[8]', 'audSpec_Rfilt_sma[9]',\n",
    "                                            'audSpec_Rfilt_sma[10]', 'audSpec_Rfilt_sma[11]', 'audSpec_Rfilt_sma[12]',\n",
    "                                            'audSpec_Rfilt_sma[13]',\n",
    "                                            'audSpec_Rfilt_sma[14]', 'audSpec_Rfilt_sma[15]', 'audSpec_Rfilt_sma[16]',\n",
    "                                            'audSpec_Rfilt_sma[17]',\n",
    "                                            'audSpec_Rfilt_sma[18]', 'audSpec_Rfilt_sma[19]', 'audSpec_Rfilt_sma[20]',\n",
    "                                            'audSpec_Rfilt_sma[21]',\n",
    "                                            'audSpec_Rfilt_sma[22]', 'audSpec_Rfilt_sma[23]', 'audSpec_Rfilt_sma[24]',\n",
    "                                            'audSpec_Rfilt_sma[25]']\n",
    "\n",
    "            if self.args['feature_type'] == 'ComParE_2016_basic_spectral':\n",
    "                feature_subset['subset'] = ['pcm_fftMag_fband250-650_sma', 'pcm_fftMag_fband1000-4000_sma',\n",
    "                                            'pcm_fftMag_spectralRollOff25.0_sma',\n",
    "                                            'pcm_fftMag_spectralRollOff50.0_sma', 'pcm_fftMag_spectralRollOff75.0_sma',\n",
    "                                            'pcm_fftMag_spectralRollOff90.0_sma', 'pcm_fftMag_spectralFlux_sma',\n",
    "                                            'pcm_fftMag_spectralCentroid_sma', 'pcm_fftMag_spectralEntropy_sma',\n",
    "                                            'pcm_fftMag_spectralVariance_sma', 'pcm_fftMag_spectralSkewness_sma',\n",
    "                                            'pcm_fftMag_spectralKurtosis_sma', 'pcm_fftMag_spectralSlope_sma',\n",
    "                                            'pcm_fftMag_psySharpness_sma', 'pcm_fftMag_spectralHarmonicity_sma']\n",
    "\n",
    "            if self.args['feature_type'] == 'ComParE_2016_llds':\n",
    "                feature_subset['subset'] = list(F.columns)\n",
    "\n",
    "            F = F[feature_subset['subset']].to_numpy()\n",
    "            F = np.nan_to_num(F)\n",
    "            F = torch.from_numpy(F).T\n",
    "\n",
    "        if self.args['feature_type'] == 'MelSpec':\n",
    "            F = self.feature_transform(s)\n",
    "\n",
    "        if self.args['feature_type'] == 'logMelSpec':\n",
    "            F = self.feature_transform(s)\n",
    "            F = torchaudio.functional.amplitude_to_DB(F, multiplier=10, amin=1e-10, db_multiplier=0)\n",
    "\n",
    "        if self.args['feature_type'] == 'MFCC':\n",
    "            F = self.feature_transform(s)\n",
    "\n",
    "        if self.args.get('compute_deltas', False):\n",
    "            FD = torchaudio.functional.compute_deltas(F)\n",
    "            F = torch.cat((F, FD), dim=0)\n",
    "\n",
    "        if self.args.get('compute_delta_deltas', False):\n",
    "            FDD = torchaudio.functional.compute_deltas(FD)\n",
    "            F = torch.cat((F, FDD), dim=0)\n",
    "        return F.T\n",
    "\n",
    "    def extract(self, filepath):\n",
    "        ''' Interface to other codes for this class\n",
    "\t\tSteps:\n",
    "\t\t1. Read audio\n",
    "\t\t2. Do feature extraction\n",
    "\t\t'''\n",
    "        self.audio_path = filepath\n",
    "        s, fs = self._read_audio(filepath)\n",
    "        return self._do_feature_extraction(s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "# Feature Extractor\n",
    "def make_feats(file_list, labels_file,feats_config):\n",
    "    # read the list of files\n",
    "    file_list = open(file_list).readlines()\n",
    "    file_list = [line.strip().split() for line in file_list]\n",
    "\n",
    "    # read labels\n",
    "    temp = open(labels_file).readlines()\n",
    "    temp = [line.strip().split() for line in temp]\n",
    "    labels = {}\n",
    "    for fil, label in temp:\n",
    "        labels[fil] = label\n",
    "    del temp\n",
    "\n",
    "    # make examples\n",
    "    egs = []\n",
    "    for fil, path in file_list:\n",
    "        # Prepare features\n",
    "        FE = FeatureExtractor(feats_config)\n",
    "        F = FE.extract(path)\n",
    "        label = labels.get(fil, None)\n",
    "        if label is not None:\n",
    "            egs.append(np.concatenate((np.array(F), np.array([label] * F.shape[0]).reshape(F.shape[0], 1)), axis=1))\n",
    "\n",
    "    egs = np.vstack(egs)\n",
    "\n",
    "    return np.array(egs[:, :-1], dtype=float), np.array(egs[:, -1], dtype=float)\n",
    "\n",
    "# Feature configuration\n",
    "feature_config = {'feature_type': 'logMelSpec',\n",
    "                  'resampling_rate': 44100,\n",
    "                  'n_mels': 64,\n",
    "                  'f_max': 22050,\n",
    "                  'window_size': 25,\n",
    "                  'hop_length': 10,\n",
    "                  'compute_deltas': True,\n",
    "                  'compute_delta_deltas': True}\n",
    "\n",
    "#TODO: MAKE SCP FROM DATAFRAME\n",
    "# Get Feats\n",
    "train_feats, train_labels = make_feats(os.path.join(path_notebooks, 'train.scp'), os.path.join(path_notebooks, 'train_labels'), feature_config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Torch utils\n",
    "def activations(act):\n",
    "    \"\"\"\n",
    "    Interface to fetch activations\n",
    "    \"\"\"\n",
    "    activ = {'Tanh': nn.Tanh(), 'ReLU': nn.ReLU(), 'Sigmoid': nn.Sigmoid()}\n",
    "    act = activ[act]\n",
    "\n",
    "    if act is not None:\n",
    "        return act\n",
    "    else:\n",
    "        raise ValueError('Unknown activation, add it in activations dictionary in models.py')\n",
    "\n",
    "class bce_loss(nn.Module):\n",
    "    \"\"\"\n",
    "    Class interface to compute BCE loss\n",
    "    Default uses mean reduction equal weight for both positive and negative samples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reduction='mean', pos_weight=torch.tensor([1])):\n",
    "        super(bce_loss, self).__init__()\n",
    "        self.criterion = nn.BCEWithLogitsLoss(reduction=reduction, pos_weight=pos_weight)\n",
    "\n",
    "    def forward(self, net_out, ref):\n",
    "        return self.criterion(net_out, ref)\n",
    "\n",
    "class FFClassificationHead(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(FFClassificationHead, self).__init__()\n",
    "\n",
    "        self.inDim = args['input_dimension']\n",
    "        self.units = [self.inDim] + [item for item in args['units'] if item > 0]\n",
    "        self.num_layers = len(self.units) - 1\n",
    "\n",
    "        self.activation_type = args['activation']\n",
    "        self.dropout_p = args['dropout']\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            setattr(self, 'linearlayer_' + str(i), nn.Linear(self.units[i], self.units[i + 1]))\n",
    "            setattr(self, 'dropout_' + str(i), nn.Dropout(self.dropout_p))\n",
    "        self.linearOut = nn.Linear(self.units[-1], 1)\n",
    "        self.activation = activations(self.activation_type)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        x = torch.vstack(inputs)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = getattr(self, 'linearlayer_' + str(i))(x)\n",
    "            x = self.activation(x)\n",
    "            x = getattr(self, 'dropout_' + str(i))(x)\n",
    "        x = self.linearOut(x)\n",
    "        return [x[i, ] for i in range(x.shape[0])]\n",
    "\n",
    "# LSTM ENCODER classifier\n",
    "class LSTMEncoder(nn.Module):\n",
    "    \"\"\" Stacked (B)LSTM Encoder\n",
    "    Arguments:\n",
    "    args: Dictionary with below entries\n",
    "    input_dimenstion: (integer), Dimension of the feature vector input\n",
    "    units: (integer), Number of LSTM units. Default: 128\n",
    "    num_layers: (integer), Number of layers in the stacked LSTM. Default: 2\n",
    "    bidirectional: (bool), if True biLSTM will be used. Default: True\n",
    "    apply_mean_norm: (bool), subtract the example level mean. Default: False\n",
    "    apply_var_norm: (bool), normalize by standard deviation. Default: False\n",
    "    pooltype: (['average' or 'last']). Default: 'average'\n",
    "    ----> 'average': average of the LSTM output along time dimension is the embedding\n",
    "    ----> 'last': LSTM hidden state at the last time-step of the last layer is the embedding\n",
    "    dropout: (float), Dropout probability. Default: 0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.inDim = args['input_dimension']\n",
    "        self.units = args.get('units', 128)\n",
    "        self.num_layers = args.get('num_layers', 2)\n",
    "        self.bidirectional = args.get('bidirectional', False)\n",
    "\n",
    "        self.apply_mean_norm = args.get('apply_mean_norm', False)\n",
    "        self.apply_var_norm = args.get('apply_var_norm', False)\n",
    "        self.dropout_p = args.get('dropout', 0)\n",
    "        assert self.dropout_p < 1\n",
    "\n",
    "        self.pooltype = args.get('pooltype', False)\n",
    "        assert self.pooltype in ['average', 'last']\n",
    "\n",
    "        self.LSTM = nn.LSTM(self.inDim,\n",
    "                            self.units,\n",
    "                            num_layers=self.num_layers,\n",
    "                            bidirectional=self.bidirectional,\n",
    "                            batch_first=True,\n",
    "                            dropout=self.dropout_p)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: a list of torch tensors\n",
    "        The tensors can be of varying length.\n",
    "        \"\"\"\n",
    "        inlens = [x.shape[0] for x in inputs]\n",
    "        if self.apply_mean_norm:\n",
    "            inputs = [F - torch.mean(F, dim=0) for F in inputs]\n",
    "        if self.apply_var_norm:\n",
    "            inputs = [F / torch.std(F, dim=0) for F in inputs]\n",
    "\n",
    "        x = pad_sequence(inputs, batch_first=True)\n",
    "        x = pack_padded_sequence(x, inlens, batch_first=True, enforce_sorted=False)\n",
    "        x, hc = self.LSTM(x)\n",
    "\n",
    "        if self.pooltype == 'average':\n",
    "            x, _ = pad_packed_sequence(x, batch_first=True)\n",
    "            x = torch.sum(x, dim=1)\n",
    "            x = torch.div(x, torch.tensor(inlens).unsqueeze(1).repeat(1, x.shape[1]).to(x.device))\n",
    "        elif self.pooltype == 'last':\n",
    "            if self.bidirectional:\n",
    "                x = hc[0][-2:, :, :].transpose(0, 1).reshape(hc[0].shape[1], 2 * hc[0].shape[2])\n",
    "            else:\n",
    "                x = hc[0][-1, :, :]\n",
    "        else:\n",
    "            raise ValueError('Unknown pooling method')\n",
    "\n",
    "        return [x[i, :].view(1, x.shape[1]) for i in range(x.shape[0])]\n",
    "\n",
    "# LSTM classifier\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Classifier architecture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        self.input_dimension = args['input_dimension']\n",
    "        self.lstm_encoder_units = args['lstm_encoder_units']\n",
    "        self.lstm_num_layers = args['lstm_num_layers']\n",
    "        self.lstm_bidirectional = args['lstm_bidirectional']\n",
    "        self.lstm_dropout_p = args['lstm_dropout']\n",
    "        self.lstm_pooling = args['lstm_pooling']\n",
    "        self.apply_mean_norm = args['apply_mean_norm']\n",
    "        self.apply_var_norm = args['apply_var_norm']\n",
    "\n",
    "        encoder_args = {'input_dimension': self.input_dimension, 'units': self.lstm_encoder_units,\n",
    "                        'num_layers': self.lstm_num_layers, 'bidirectional': self.lstm_bidirectional,\n",
    "                        'apply_mean_norm': self.apply_mean_norm, 'apply_var_norm': self.apply_var_norm,\n",
    "                        'dropout': self.lstm_dropout_p, 'pooltype': self.lstm_pooling}\n",
    "\n",
    "        self.encoder = LSTMEncoder(encoder_args)\n",
    "\n",
    "        temp = args['classifier_units']\n",
    "        if type(temp) == list:\n",
    "            self.classifier_units = temp\n",
    "        else:\n",
    "            self.classifier_units = [temp]\n",
    "        self.classifier_activation = args['classifier_activation']\n",
    "        self.classifier_dropout_p = args['classifier_dropout']\n",
    "        cls_idim = 2 * self.lstm_encoder_units if self.lstm_bidirectional else self.lstm_encoder_units\n",
    "        classifier_args = {'input_dimension': cls_idim, 'units': self.classifier_units,\n",
    "                           'dropout': self.classifier_dropout_p, 'activation': self.classifier_activation}\n",
    "\n",
    "        self.classifier = FFClassificationHead(classifier_args)\n",
    "        self.criterion = bce_loss()\n",
    "\n",
    "    def init_encoder(self, params):\n",
    "        \"\"\"\n",
    "        Initialize the feature encoder using a pre-trained model\n",
    "        \"\"\"\n",
    "        self.encoder.load_state_dict(params)\n",
    "\n",
    "    def init_classifier(self, params):\n",
    "        \"\"\"\n",
    "        Initialize the classification-head using a pre-trained classifier model\n",
    "        \"\"\"\n",
    "        self.classifier.load_state_dict(params)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\"\n",
    "        Prediction of the classifier score\n",
    "        \"\"\"\n",
    "        return self.classifier(self.encoder(inputs))\n",
    "\n",
    "    def predict_proba(self, inputs):\n",
    "        \"\"\"\n",
    "        Prediction of the posterior probability\n",
    "        \"\"\"\n",
    "        return [torch.sigmoid(item) for item in self.predict(inputs)]\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Forward pass through the network and loss computation\n",
    "        \"\"\"\n",
    "        return self.criterion(torch.stack(self.predict(inputs)), torch.stack(targets))\n",
    "\n",
    "# Models configurations\n",
    "ALL_MODELS = {'LogisticRegression': {'c': 0.01,\n",
    "                                     'max_iter': 40,\n",
    "                                     'solver': 'liblinear',\n",
    "                                     'penalty': 'l2',\n",
    "                                     'class_weight': 'balanced', 'random_state': SEED, 'verbose': True},\n",
    "              'RandomForest': {'n_estimators': 20,\n",
    "                               'criterion': 'gini',\n",
    "                               'max_depth': None,\n",
    "                               'min_samples_split': 2,\n",
    "                               'min_samples_leaf': 1,\n",
    "                               'max_features': 'sqrt',\n",
    "                               'class_weight': 'balanced', 'random_state': SEED, 'verbose': True},\n",
    "              'MLP': {'learning_rate_init': 0.001, 'alpha': 0.001, 'solver': 'adam', 'hidden_layer_sizes': [20, 20], 'max_iter': 500, 'activation': 'tanh',\n",
    "                      'class_weight': 'balanced', 'random_state': SEED, 'verbose': True},\n",
    "              'linearSVM': {'penalty': 'l2',\n",
    "                            'loss': 'squared_hinge',\n",
    "                            'c': 0.01,\n",
    "                            'tol': 1e-4,\n",
    "                            'max_iter': 100,\n",
    "                            'class_weight': 'balanced', 'random_state': SEED, 'verbose': True}\n",
    "              }\n",
    "\n",
    "# Training function\n",
    "def config_model(model_name, training_feats, training_labels):\n",
    "\n",
    "    model_args = ALL_MODELS[model_exp]\n",
    "\n",
    "    if model_name == 'LogisticRegression':\n",
    "        model = LogisticRegression(C=float(model_args['c']),\n",
    "                                   max_iter=int(model_args['max_iter']),\n",
    "                                   solver=model_args['solver'],\n",
    "                                   penalty=model_args['penalty'],\n",
    "                                   class_weight=model_args['class_weight'],\n",
    "                                   random_state=model_args['random_state'],\n",
    "                                   verbose=True)\n",
    "\n",
    "    elif model_name == 'RandomForest':\n",
    "        model = RandomForestClassifier(n_estimators=model_args['n_estimators'],\n",
    "                                       criterion=model_args['criterion'],\n",
    "                                       max_depth=model_args['max_depth'],\n",
    "                                       min_samples_split=model_args['min_samples_split'],\n",
    "                                       min_samples_leaf=model_args['min_samples_leaf'],\n",
    "                                       max_features=model_args['max_features'],\n",
    "                                       class_weight=model_args['class_weight'],\n",
    "                                       random_state=model_args['random_state'])\n",
    "\n",
    "    elif model_name == 'LinearSVM':\n",
    "        model = SVC(penalty=model_args['penalty'],\n",
    "                    loss=model_args['loss'],\n",
    "                    C=model_args['c'],\n",
    "                    tol=model_args['tol'],\n",
    "                    max_iter = model_args['max_iter'],\n",
    "                    verbose=model_args['verbose'],\n",
    "                    class_weight=model_args['class_weight'],\n",
    "                    random_state=model_args['random_state'])\n",
    "\n",
    "    elif model_name == 'MLP':\n",
    "        model = MLPClassifier(hidden_layer_sizes=model_args['hidden_layer_sizes'],\n",
    "                              solver=model_args['solver'], alpha=model_args['alpha'],\n",
    "                              learning_rate_init=model_args['learning_rate_init'],\n",
    "                              verbose=model_args['verbose'], activation=model_args['activation'],\n",
    "                              max_iter=model_args['max_iter'], random_state=model_args['random_state'])\n",
    "\n",
    "        if model_args['class_weight'] == 'balanced':\n",
    "            train_data = np.concatenate((training_feats, training_labels.reshape(training_feats.shape[0], 1)), axis=1)\n",
    "            ind = np.where(train_data[:, -1] == 1)[0]\n",
    "            n_positives = len(ind)\n",
    "            n_negatives = train_data.shape[0] - n_positives\n",
    "            up_sample_factor = int(n_negatives / n_positives) - 1\n",
    "            for i in range(up_sample_factor):\n",
    "                train_data = np.concatenate((train_data, train_data[ind, :]), axis=0)\n",
    "            np.random.shuffle(train_data)\n",
    "            training_feats = train_data[:, :-1]\n",
    "            training_labels = train_data[:, -1]\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Not implementation of the model: \" + model_exp)\n",
    "    return model, training_feats, training_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "#Select a model\n",
    "model_exp = 'LogisticRegression'\n",
    "# Start training\n",
    "model, x_train, y_train  = config_model(model_exp, train_feats, train_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]{'roc_auc': [1.0, 1.0, 0.9976744186046511, 1.0, 0.9995210727969349], 'accuracy': [0.9910714285714286, 0.990990990990991, 0.9819819819819819, 0.990990990990991, 0.990990990990991], 'f1': [0.9803921568627451, 0.9803921568627451, 0.9615384615384615, 0.9795918367346939, 0.9795918367346939], 'precision': [0.9615384615384616, 0.9615384615384616, 0.9259259259259259, 1.0, 0.96], 'recall': [1.0, 1.0, 1.0, 0.96, 1.0]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "METRICS = ['roc_auc', 'accuracy', 'f1', 'precision', 'recall']\n",
    "cv = StratifiedKFold(n_splits=K_NUM, random_state=SEED, shuffle=True)\n",
    "\n",
    "scores = {}\n",
    "for metric in METRICS:\n",
    "    scores[metric] = list(cross_val_score(model, x_train, y_train, scoring=metric, cv=cv))\n",
    "\n",
    "print(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}